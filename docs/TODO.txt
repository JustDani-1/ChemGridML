
- get final performance metrics RRMSE and AUROC for best hyperparams
    - compare performance with and without early stopping on final model
    - compare performance with and without patience as a tunable hyperparam
- add a single parameter for different preset fp x model x data as well as hyperparams space options
- get myriad pipeline working, run and evaluate first job
- function to generate benchmark just from the study

IDEAS:
(Kamuntaviƒçius et al.)
----------------------
- test performance on data from a different source to really compare the generalization 
(Deng et al.)
-------------
- test predicting performance on simple computable descriptors like MolWT or N_Atoms
- inter-scaffold and intra-scaffold generalization:
    - inter: compound bias, move molecules from the same scaffold into the same fold
    - intra: activity cliff, very similar mols from the same scaffold have significant property value difference
- ask Claude or research; activity cliffs:
    - can we somehow include the target/task into the model?
    - could help with activity cliffs because the model can see what molecule/target the task is for
- cross-validation on best hyperparams, which leads basically to e.g 10 AUROC value
    - perform t-test to determine p value for hypothesis that models have the same mean performance
    - if p < 0.05, the hypo is rejected and we can say with statistical certainty than one model is better
- ensembling techniques have not been evaluated in this study
- aspect of explainability of models not covered
- they use 80:10:10 split, and 30 (0..29) seeds for statistical stability

- CV: 80:10:10 
    - use 80% to train, 10% val to assess for hyperparam optim
    - after hyperparam optim, the final performance is measured on 10% test
    - repeat 30 times with different seed for 30 optimal hyperparams and 30 metric scores

Mine:
-----
- let the study just output the raw best hyperparams or predictions
- then, you can run seperate eval jobs to compute different metrics
- STRATEGY:
    - just go through with your TDC model expirement
    - try to incorporate learnable and pre-trained approaches for the first big test
    - also use 80:10:10 with different seeds
    - somehow save the raw results, then compute metrics and plots locally
    - after that go back into refining the problem or just assess the quality of your results
- TRAINING STRATEGY:
    - two phases: hyperparam optim, then final evaluation phase
    - perform optuna study with CV on 85:15 train-val split
    -> optain "optimal" hyperparams
    - use k=10 different seeds to generate 85:15 train-test splits (random or scaffold)
    -> optain k final performance predictions from with metrics can be calculated



TODAYS GOAL:
- write a myriad job that runs a job 
- use Scaler always in the right manner (just on train)
- sklearn also uses custom KFold, returns raw prediction for when each data point was in validation set
- then apply problem-specific loss to it

- try to convert with the same API as the other fingerprints to convert via GROVER, measure performance
- try to convert 4 TDC datasets with GROVER, test performance
- if not too long then acchieve the same API as with the other fingerprints
- make it so that each fingerprint x datset combination is computed only once and then cached in memory


- new methods!!! pre-trained and learnable representations
- molbert and GROVER
- nested CV, launch fingerprint expirement on real scale to get first usable results
- write script that gets me the combinations of the 10 jobs that took the longest
- multi-threading in outer loop of nested CV, maybe because some of the combinations are taking a long time
- standardscaler must be only trained on one, but applied to both

- maybe static method for every model to define data preprocessing pipeline or maybe as part of the dataset depending on fingerprint

- look into why the FNN outperforms always on the classification tasks

- 18 <-> ECFP SVM Caco2_Wang went through but is not in the benchmark

SVM doesnt terminate: MOL2VEC SVM Caco2_Wang => need to log inside the optuna trials to see wether progress is made

FOR EACH COMBO
old approach:
N_TRIALS * N_FOLDS + N_TESTS = 50 * 5 + 15 = 265

new approach(nested CV):
N_TRIALS * N_FOLDS * N_TESTS = 20 * 5 * 5 = 250

Presentation for Mark:
- present the papers
- quickly show MoleculeNet
- ask him in which direction the project should go? 
- molecular target or property prediction?
- go nuts with the featurizations or with the model types?
- WHAT IS THE DIRECTION OR QUESTION I AM TRYING TO ANSWER?
- use data and models like in Mayr but with the diversity of descriptors from the others?
- More diverse architectures: FNN, RNN, KNN, NB, SVM, RF, ...
-> would result in A LOT of models/computation


Follow-up:
- focus on combination of representation x model x dataset
- property prediction, not target prediction
- try to use curated datasets at first and then maybe try to use ChEMBL-extracted datasets, compare performance
    - compare performances over small, medium and large datasets
- features -> models -> datasets




Methods:
- We want a vectorization of all features
- Graphs:
    - test just plain GCN combined with other architectures
    - graphical autoencoder with other architectures, 
    - extract embedding from traditional GCN, combine with other architectures




