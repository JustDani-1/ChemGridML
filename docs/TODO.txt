
- get final performance metrics RRMSE and AUROC for best hyperparams
    - compare performance with and without early stopping on final model
    - compare performance with and without patience as a tunable hyperparam
- add a single parameter for different preset fp x model x data as well as hyperparams space options
- get myriad pipeline working, run and evaluate first job
- function to generate benchmark just from the study

IDEAS:
(Kamuntaviƒçius et al.)
----------------------
- test performance on data from a different source to really compare the generalization 
(Deng et al.)
-------------
- test predicting performance on simple computable descriptors like MolWT or N_Atoms
- inter-scaffold and intra-scaffold generalization:
    - inter: compound bias, move molecules from the same scaffold into the same fold
    - intra: activity cliff, very similar mols from the same scaffold have significant property value difference
- ask Claude or research; activity cliffs:
    - can we somehow include the target/task into the model?
    - could help with activity cliffs because the model can see what molecule/target the task is for
- cross-validation on best hyperparams, which leads basically to e.g 10 AUROC value
    - perform t-test to determine p value for hypothesis that models have the same mean performance
    - if p < 0.05, the hypo is rejected and we can say with statistical certainty than one model is better
- ensembling techniques have not been evaluated in this study
- aspect of explainability of models not covered
- they use 80:10:10 split, and 30 (0..29) seeds for statistical stability

- CV: 80:10:10 
    - use 80% to train, 10% val to assess for hyperparam optim
    - after hyperparam optim, the final performance is measured on 10% test
    - repeat 30 times with different seed for 30 optimal hyperparams and 30 metric scores

Mine:
-----
- let the study just output the raw best hyperparams or predictions
- then, you can run seperate eval jobs to compute different metrics
- STRATEGY:
    - just go through with your TDC model expirement
    - try to incorporate learnable and pre-trained approaches for the first big test
    - also use 80:10:10 with different seeds
    - somehow save the raw results, then compute metrics and plots locally
    - after that go back into refining the problem or just assess the quality of your results
- TRAINING STRATEGY:
    - two phases: hyperparam optim, then final evaluation phase
    - perform optuna study with CV on 85:15 train-val split
    -> optain "optimal" hyperparams
    - use k=10 different seeds to generate 85:15 train-test splits (random or scaffold)
    -> optain k final performance predictions from with metrics can be calculated



TODAYS GOAL:
- write a myriad job that runs a job 
- use Scaler always in the right manner (just on train)
- sklearn also uses custom KFold, returns raw prediction for when each data point was in validation set
- then apply problem-specific loss to it

- try to convert with the same API as the other fingerprints to convert via GROVER, measure performance
- try to convert 4 TDC datasets with GROVER, test performance
- if not too long then acchieve the same API as with the other fingerprints
- make it so that each fingerprint x datset combination is computed only once and then cached in memory


- new methods!!! pre-trained and learnable representations
- molbert and GROVER

- maybe static method for every model to define data preprocessing pipeline or maybe as part of the dataset depending on fingerprint

- look into why the FNN outperforms always on the classification tasks

- wrap basically everything in the model class 
    - .preprocess => scaling and making to tensor
    - .fit => fitting the model (for pytorch, the epochs are an attribute)
    - .predict => predict from trained model
    => 3 base classes: deepchem, sklearn, pytorch

- for the 10 scores for each model, perform statistical significance tests
- try out label scaling for the regression tasks
- use all the deepchem fingerprints instead of RdKit, check if they are the same e.g. for MOL2VEC

- check wether the classification outputs of sklearn are 0|1 => dont just apply BCE-logits-loss to them
- deepchem outputs probability for each class => also dont just apply BCE-logits

- try out MolBERT and GROVER, for now just as featurizors; then do an end-to-end ML approach where you concatenate a MLP
- somehow encode which features are possible for which model so that the entire expirement can be run

- GROVER: concatenate 200 dim RdKit descriptors with learned representations
    - when trying to use GROVER not as FP but with finetuning, the entire READOUT + 2 MLPs have to be optimized

- write get_num and get_combo for new methods abstraction
- update benchmark manager to produce different plot that compares "methods" 

- write hierarchical submit script with
    - master.py [FINGERPRINT] [LEARNABLE] [PRETRAINED] [TDC|DATASIZE] 
    - determines total tasks and produces submit-scripts
    - maybe one job-array per category, requesting more or less resources each, or GPU
    - include benchmark_manager afterwards, to produce graphics according to the configuration

master idea:
    - save everything under job_id of the master
    - combine output files into single logits
    - save them in ./output/<master_job_id> under Job.o<master_job_id>.<task_id>
    - pass master job id to job tasks, so that the studies also get saved under ./studies/<master_job_id>/studies
    - define "experiments": sets of methods (include feature, model, datset) as well as required resources
    - then in the master script just define which experiments to run
        - FINGERPRINT, LEARNABLE, PRETRAINED, DATASIZE (directly with determined subset of methods on different datasets)
    


TIMELINE
========================================================================================================
========================================================================================================

Week 5:
    - get_num, get_combo
    - start on master.py and submit_master.sh

Week 6:
    - finish master.py/.sh
    - smart job submitting, depending on the experiment
    - include MolBERT and GROVER
    - make final decision which fingerprints and which graph models to include
    - make final evaluation of Fingerprint performances, decide on "winners"
        -> small winner set and a little extra will be included into datasize experiment
    - write Myriad efficiency script based on Mark's runtimes

Week 7:
    - datasize experiment 
    - begin write up, prepare Presentation
    - maybe support for multi-task problems
    - different dataset-splits
    - evaluation of data, writing nice benchmark_manager for everything

Week 8:
    - prepare presentation

========================================================================================================
========================================================================================================



FOR EACH COMBO
old approach:
N_TRIALS * N_FOLDS + N_TESTS = 50 * 5 + 15 = 265

new approach(nested CV):
N_TRIALS * N_FOLDS * N_TESTS = 20 * 5 * 5 = 250

Presentation for Mark:
- present the papers
- quickly show MoleculeNet
- ask him in which direction the project should go? 
- molecular target or property prediction?
- go nuts with the featurizations or with the model types?
- WHAT IS THE DIRECTION OR QUESTION I AM TRYING TO ANSWER?
- use data and models like in Mayr but with the diversity of descriptors from the others?
- More diverse architectures: FNN, RNN, KNN, NB, SVM, RF, ...
-> would result in A LOT of models/computation


Follow-up:
- focus on combination of representation x model x dataset
- property prediction, not target prediction
- try to use curated datasets at first and then maybe try to use ChEMBL-extracted datasets, compare performance
    - compare performances over small, medium and large datasets
- features -> models -> datasets


Methods:
- We want a vectorization of all features
- Graphs:
    - test just plain GCN combined with other architectures
    - graphical autoencoder with other architectures, 
    - extract embedding from traditional GCN, combine with other architectures



