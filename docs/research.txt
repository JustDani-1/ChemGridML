Baptista et al (13.08.):
------------------------
Goal:
- compare the performance of learnable representations and computational representations / fingerprints
-> fingerprint vs. end-to-end DL
- start with the claim, that the problem determines the best representation

Method:
- narrow down the problem: predicting drug sensitivity in cancer cell lines
- 12 representations, 5 datasets
- after the featurization, a standard FNN was used
- hyperparameter optim. via 5fold CV with stratified split
- classification with BCE loss & AUROC metric, regression with MSE loss & RMSE metric

Result:
- fp vs. e-t-e DL perform similar, with the best representation depending mostly on the dataset itself

Representations:
- ECFP4
	- D=1024
- ECFP6
	- D=1024
- MACCS
	- D=166
- AtomPair
	- topological fingerprint based on the shortest distance between all pairs of atoms
	- D=1024
- RDKitFP
	- topological fingerprint based on subgraphs
	- binary vector of fixed length
	- D=1024
- LayeredRDKitFP
	- same as RDKitFP, but introduces layering mechanism
	- D=1024
- Mol2Vec
- TextCNN
	- 1D CNN, originally for sentence classification
	- uses tokenized and one-hot encoded SMILES
	- several 1C convolutional filters, followed my a max-over-time pooling operation
- GraphConv (Neural Fingerprints)
- GCN (Graph Convolutional Neural network)
- GAT (Graph Attention Network)
- AttentiveFP


Stepisnik et al (13.08.):
-------------------------
Goal:
- expert (fingerprints/numeric properties) vs. learnable (task-indep=prelearned/task-dep=learnable)
- pair-wise combination of representations

Method:
- 8 representations, 11 datasets
- after the featurization, a standard FNN and PCTs were used
- classification with BCE loss & AUROC metric, regression with MSE loss & RRMSE metric

Result:
- similar performance, problem specific

Representations:
- ECFP (Extended-Connectivity Fingerprints)
	- identifier for each atom that gets updated iteratively through neighborhood
	- radius parameter
	- identifiers then get hashed into parametrically-determined-length binary vector
- MACCS (Molecular ACCess System)
	- binary vector of length 166
	- presence/absence of predetermined patterns or substructure
- PaDEL (Molecular Descriptors)
	- calculated (physical) properties like molWeight, atom count, charge descriptors, ...
	- calculated using PaDEL software
	- 1444 2D properties, 431 3D properties = 1875 total
- Spectrophores
	- calculated from 3D atomic properties
	- physical properties
	- vector of 48 real numbers
- Mol2vec
	- learn representations, from substructures
	- model that was pre-trained on ZINC database
	- vector of 300 real numbers
- CDDD (Continuous data-driven descriptors)
	- calculated with RNN autoencoder
	- model was pre-trained on ZINC database
	- input are SMILES, output vector of 512 real numbers
- GCN (Graph Convolutional Neural network)
	- continuous analogue of ECFP
	- learn task-specific molecular features
- Weave models
	- similar to GCN
	- for updating the features of an atom, take all other atoms into account, not just neighbors
	- computationally expensive
	- learn task-specific features
	


Mayr et al (13.08.):
--------------------
Goal:
- compare machine learning methods for drug target prediction on ChEMBL

Method:
- compound series bias
	- compounds are tested in series with many very similar compounds
	- makes it easier for the model, if the series is both in the train and test set
	-> cluster data and then always include entire clusters into folds
	- single-linkage clustering with on Jaccard distances of the ECFP4, with minD=0.3 
- FNN, SVM, SmilesLSTM, GCN, RF (Rand Forr), Weave, KNN (K-Nearest), NB (Naive Bayes), SEA (Sim. Ens. Appr.)
- used different features, mostly computational

Result:

Representations:
- StaticF
	- common computed static features
	- charge, geometry, connectivity, autocorrelation, ...
- SemiF
	- common semisparse features
	- CATS2D, MACCS, PCFP, SHED, ...
- ToxF
	- 2,276 in-house toxicophore features
- ECFP
	- rad=2 and rad=3
	- DFS8


Concepts:
---------
- one strategy for self-supervised learning of molecular embeddings:
	- remove bonds/substructures, let model predict what should be there
	

Claude:

Your categorization makes good conceptual sense and captures the main paradigms in molecular property prediction! The three-way division based on modularity and training approach is logical. Here are some thoughts on your structure:
Strengths of your categorization:

Clear distinction between fixed vs. learnable representations
Captures the important concept of modularity (separable featurization vs. end-to-end)
Recognizes the emergence of pre-trained molecular models as a distinct paradigm

Potential refinements to consider:

Hybrid approaches: Some methods blur the lines between your categories. For example:

Graph neural networks that incorporate traditional fingerprints as node/edge features
Models that combine multiple representation types (e.g., SMILES + graph + 3D structure)
Transfer learning approaches where category 2 models are pre-trained then fine-tuned


3D structure-based methods: You might want to explicitly address whether 3D conformational methods fit into your scheme:

Traditional 3D descriptors (like RDF, WHIM) would go in category 1
End-to-end 3D models (like SchNet, DimeNet) would go in category 2
Pre-trained 3D models would go in category 3


Sequence-based representations: Beyond SMILES, there are other sequence representations (SELFIES, InChI) that could be worth noting.
Multi-modal pre-trained models: Some newer pre-trained models (like MolCLIP) learn joint representations across multiple modalities.

Suggested minor adjustments:

Consider renaming category 2 to "End-to-end learnable representations" for clarity
You might note that some category 3 models can also be fine-tuned end-to-end, making them somewhat hybrid

Overall, your framework provides a solid foundation for organizing the molecular featurization landscape. The key insight about modularity vs. end-to-end learning is particularly valuable for understanding trade-offs in model design and computational requirements.